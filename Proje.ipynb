{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "deafe713-6517-4025-a4a3-9501d5648f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'dbmdz/bert-base-turkish-cased' için tokenizer yükleniyor...\n",
      "Tokenizer başarıyla yüklendi.\n",
      "'dbmdz/bert-base-turkish-cased' modeli yükleniyor ve sınıflandırma başlığı ekleniyor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model başarıyla yüklendi.\n",
      "GPU (CUDA) algılandı. Model GPU'ya taşınıyor...\n",
      "Model şu cihazda: cuda:0\n",
      "Kullanılan GPU: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n",
      "\n",
      "--- Yükleme Kontrolü ---\n",
      "Yüklenen Model Adı: dbmdz/bert-base-turkish-cased\n",
      "Yüklenen Tokenizer Tipi: <class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>\n",
      "Yüklenen Model Tipi: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>\n",
      "Modelin şu anki cihazı: cuda:0\n",
      "\n",
      "Örnek Metin: 'Yapay zeka modelleri, metinleri analiz etmekte çok başarılıdır.'\n",
      "Token'lar: ['[CLS]', 'Yapay', 'zeka', 'modelleri', ',', 'metin', '##leri', 'analiz', 'etmekte', 'çok', 'başarılı', '##dır', '.', '[SEP]']\n",
      "Input IDs: tensor([[    2, 27980, 14348,  6690,    16,  8111,  2065,  6516, 11532,  2140,\n",
      "          4165,  2077,    18,     3]])\n",
      "Giriş tensorleri şu cihaza taşındı: cuda:0\n",
      "\n",
      "Model Çıkışı (Logitler): tensor([[-0.3814,  0.4137]], device='cuda:0')\n",
      "Model Çıkışı (Olasılıklar): tensor([[0.3111, 0.6889]], device='cuda:0')\n",
      "Tahmini Sınıf İndeksi (0 veya 1): 1\n"
     ]
    }
   ],
   "source": [
    "# Gerekli kütüphaneleri içe aktar\n",
    "import pandas as pd # Veri manipülasyonu için Pandas kütüphanesi\n",
    "import torch # Derin öğrenme operasyonları için PyTorch kütüphanesi\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification # Hugging Face Transformers kütüphanesinden tokenizer ve sınıflandırma modeli sınıfları\n",
    "\n",
    "# 1. Adım: Model Adını Seç\n",
    "# Kullanılacak önceden eğitilmiş BERT modelinin adı belirlenir.\n",
    "# \"dbmdz/bert-base-turkish-cased\" Türkçe için optimize edilmiş bir modeldir.\n",
    "model_name = \"dbmdz/bert-base-turkish-cased\" \n",
    "\n",
    "# 2. Adım: Tokenizer'ı Yükle\n",
    "print(f\"'{model_name}' için tokenizer yükleniyor...\")\n",
    "# Belirlenen model adına uygun tokenizer yüklenir.\n",
    "# Tokenizer, metinleri modelin anlayabileceği sayısal formata (token ID'lerine) dönüştürür.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(\"Tokenizer başarıyla yüklendi.\")\n",
    "\n",
    "# 3. Adım: Modeli Yükle\n",
    "print(f\"'{model_name}' modeli yükleniyor ve sınıflandırma başlığı ekleniyor...\")\n",
    "# Belirlenen model adıyla önceden eğitilmiş model yüklenir.\n",
    "# `num_labels=2` parametresi, modelin ikili sınıflandırma (doğru/yanlış) yapacağını belirtir.\n",
    "# Bu, modelin üzerine yeni bir sınıflandırma katmanı eklenmesini sağlar.\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "print(\"Model başarıyla yüklendi.\")\n",
    "\n",
    "# Modeli GPU'ya taşıma kontrolü ve taşıma\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU (CUDA) algılandı. Model GPU'ya taşınıyor...\")\n",
    "    model.to('cuda') # Modeli GPU'ya taşı\n",
    "    print(f\"Model şu cihazda: {model.device}\")\n",
    "    # Kullanılan GPU'nun adını da yazdırabiliriz\n",
    "    print(f\"Kullanılan GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU (CUDA) algılanmadı. Model CPU'da kalacak.\")\n",
    "\n",
    "# 4. Adım: Yüklemeyi Kontrol Et (İsteğe bağlı)\n",
    "print(\"\\n--- Yükleme Kontrolü ---\")\n",
    "print(f\"Yüklenen Model Adı: {model_name}\")\n",
    "print(f\"Yüklenen Tokenizer Tipi: {type(tokenizer)}\")\n",
    "print(f\"Yüklenen Model Tipi: {type(model)}\")\n",
    "print(f\"Modelin şu anki cihazı: {model.device}\") # Modelin hangi cihazda olduğunu kontrol et\n",
    "\n",
    "# Örnek bir metni kullanarak tokenizer'ın ve modelin çalıştığını göster\n",
    "sample_text = \"Yapay zeka modelleri, metinleri analiz etmekte çok başarılıdır.\"\n",
    "# Örnek metin tokenizer ile işlenir.\n",
    "# `return_tensors=\"pt\"` çıktının PyTorch tensor'u olmasını sağlar.\n",
    "# `truncation=True` uzun metinleri modelin maksimum giriş uzunluğuna göre keser.\n",
    "# `padding=True` kısa metinleri aynı uzunluğa kadar doldurur.\n",
    "inputs = tokenizer(sample_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "print(f\"\\nÖrnek Metin: '{sample_text}'\")\n",
    "# Tokenizer tarafından metnin ayrıştırıldığı token'ları (kelime parçalarını) gösterir.\n",
    "print(f\"Token'lar: {tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])}\")\n",
    "# Token'ların sayısal ID'lerini gösterir.\n",
    "print(f\"Input IDs: {inputs['input_ids']}\")\n",
    "\n",
    "# Eğer model GPU'daysa, input'ları da GPU'ya taşı\n",
    "if model.device.type == 'cuda':\n",
    "    inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "    print(f\"Giriş tensorleri şu cihaza taşındı: {inputs['input_ids'].device}\")\n",
    "\n",
    "# Modelden tahmin al (model henüz veri setinle eğitilmediği için anlamsız bir tahmin dönecektir)\n",
    "# `torch.no_grad()` bağlam yöneticisi, tahmin yaparken gradyan hesaplamasını kapatır.\n",
    "# Bu, bellek ve hesaplama açısından verimlilik sağlar çünkü eğitim yapılmamaktadır.\n",
    "with torch.no_grad(): \n",
    "    outputs = model(**inputs) # Modelden çıktı alınır\n",
    "\n",
    "logits = outputs.logits # Modelin ham çıktıları (logitler)\n",
    "# Logitler softmax fonksiyonundan geçirilerek olasılıklara dönüştürülür.\n",
    "# `dim=1` olasılıkların sınıf boyutunda hesaplanmasını sağlar.\n",
    "probabilities = torch.softmax(logits, dim=1) \n",
    "\n",
    "print(f\"\\nModel Çıkışı (Logitler): {logits}\")\n",
    "print(f\"Model Çıkışı (Olasılıklar): {probabilities}\")\n",
    "# En yüksek olasılığa sahip sınıfın indeksini bulur (0 veya 1).\n",
    "print(f\"Tahmini Sınıf İndeksi (0 veya 1): {torch.argmax(probabilities, dim=1).item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f993302a-5b45-4e5f-83aa-3ee0094e2f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veri Seti Başarıyla Yüklendi!\n",
      "\n",
      "Veri setinin ilk 5 satırı:\n",
      "    type                                               news  \\\n",
      "0  real  Gazze’de sağlanan ateşkes sonrası Suriye'nin b...   \n",
      "1  real  Hamas ve İsrail arasında anlaşma sağlandığının...   \n",
      "2  real  Gazze'de Hamas ile İsrail arasında varılan ate...   \n",
      "3  real  Hamas ve İsrail arasında anlaşma sağlandığının...   \n",
      "4  real  İsrail’in Gazze’ye düzenlediği saldırılarda ya...   \n",
      "\n",
      "          shared_site_name source  \n",
      "0  https://t.co/GVwaGyT1lG     AA  \n",
      "1  https://t.co/omyAR4NXll     AA  \n",
      "2  https://t.co/UkQJnOnWSW     AA  \n",
      "3  https://t.co/afEqmk38Pp     AA  \n",
      "4  https://t.co/xjSuhkjOp5     AA  \n",
      "\n",
      "Veri setinin sütunları: ['type', 'news', 'shared_site_name', 'source']\n",
      "--------------------------------------------------\n",
      "Metin sütunu olarak 'news' kullanılıyor.\n",
      "Etiket sütunu olarak 'type' kullanılıyor.\n",
      "--------------------------------------------------\n",
      "\n",
      "Veri temizleme sonrası DataFrame boyutu: 693 satır.\n",
      "DataFrame'in ilk 3 satırı (temizlenmiş):\n",
      "                                                 text  labels\n",
      "0  gazzede sağlanan ateşkes sonrası suriyenin baş...       0\n",
      "1  hamas ve israil arasında anlaşma sağlandığının...       0\n",
      "2  gazzede hamas ile israil arasında varılan ateş...       0\n",
      "Etiket dağılımı (temizlenmiş):\n",
      " labels\n",
      "0    357\n",
      "1    336\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Eğitim seti boyutu: 560\n",
      "Doğrulama (Validation) seti boyutu: 63\n",
      "Test seti boyutu: 70\n",
      "\n",
      "Eğitim seti etiket dağılımı:\n",
      " labels\n",
      "0    289\n",
      "1    271\n",
      "Name: count, dtype: int64\n",
      "Doğrulama seti etiket dağılımı:\n",
      " labels\n",
      "0    32\n",
      "1    31\n",
      "Name: count, dtype: int64\n",
      "Test seti etiket dağılımı:\n",
      " labels\n",
      "0    36\n",
      "1    34\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Hugging Face Dataset formatında eğitim seti:\n",
      " Dataset({\n",
      "    features: ['text', 'labels'],\n",
      "    num_rows: 560\n",
      "})\n",
      "Hugging Face Dataset formatında doğrulama seti:\n",
      " Dataset({\n",
      "    features: ['text', 'labels'],\n",
      "    num_rows: 63\n",
      "})\n",
      "Hugging Face Dataset formatında test seti:\n",
      " Dataset({\n",
      "    features: ['text', 'labels'],\n",
      "    num_rows: 70\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # Veri manipülasyonu ve analizi için Pandas kütüphanesi\n",
    "from sklearn.model_selection import train_test_split # Veri setini eğitim, test vb. parçalara ayırmak için\n",
    "from datasets import Dataset # Hugging Face'in optimizasyonlu veri seti sınıfı\n",
    "import re # Metin temizleme (regular expressions) işlemleri için\n",
    "\n",
    "# 1. Adım: Veri setini yükle\n",
    "# İndirilen 'fake_news.csv' dosyasının yolu belirtilir ve Pandas ile DataFrame olarak okunur.\n",
    "file_path = 'fake_news.csv'\n",
    "\n",
    "try:\n",
    "    # Dosya okuma sırasında kodlama hatası olmaması için deneme-yanılma yapalım\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding='utf-8') # Önce yaygın olan utf-8 kodlamasını deneyelim\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(file_path, encoding='latin1') # utf-8 olmazsa latin1 kodlamasını deneyelim\n",
    "        print(\"Dosya UTF-8 yerine 'latin1' kodlamasıyla yüklendi.\")\n",
    "    except Exception as e: # Diğer olası hataları yakala\n",
    "        print(f\"Dosya okunurken beklenmeyen bir hata oluştu: {e}\")\n",
    "        raise # Hatayı yeniden fırlat ki dış except blokları yakalayabilsin\n",
    "\n",
    "    print(\"Veri Seti Başarıyla Yüklendi!\\n\")\n",
    "\n",
    "    # İLK KONTROL: Veri setinin ilk birkaç satırını ve sütun adlarını göstererek yapısını kontrol et\n",
    "    print(\"Veri setinin ilk 5 satırı:\\n\", df.head())\n",
    "    print(\"\\nVeri setinin sütunları:\", df.columns.tolist()) # .tolist() ekledim daha okunaklı olsun\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # 'fake_news.csv' dosyası için beklenen sütun adları: 'news' ve 'type'\n",
    "    text_column = 'news'\n",
    "    label_column = 'type'\n",
    "\n",
    "    # Sütunların gerçekten var olup olmadığını kontrol et\n",
    "    if text_column not in df.columns:\n",
    "        raise ValueError(f\"'{text_column}' sütunu veri setinde bulunamadı. Lütfen dosya yapısını kontrol edin.\")\n",
    "    if label_column not in df.columns:\n",
    "        raise ValueError(f\"'{label_column}' sütunu veri setinde bulunamadı. Lütfen dosya yapısını kontrol edin.\")\n",
    "\n",
    "    print(f\"Metin sütunu olarak '{text_column}' kullanılıyor.\")\n",
    "    print(f\"Etiket sütunu olarak '{label_column}' kullanılıyor.\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n",
    "    # 2. Adım: Metinleri temizle\n",
    "    # Metin ön işleme için bir fonksiyon tanımlanır.\n",
    "    def clean_text(text):\n",
    "        if not isinstance(text, str):\n",
    "            return \"\" # Metin olmayan değerleri (örn. NaN) boş stringe çevirerek hata oluşmasını engeller.\n",
    "        text = text.lower() # Tüm metni küçük harfe dönüştürerek büyük/küçük harf farkını ortadan kaldırır.\n",
    "        text = re.sub(r'http\\S+|www\\S+|t\\.co/\\S+', '', text) # URL'leri metinden kaldırır.\n",
    "        text = re.sub(r'[^\\w\\s]', '', text) # Alfabetik ve boşluk karakteri dışındaki tüm noktalama işaretlerini kaldırır.\n",
    "        text = re.sub(r'\\s+', ' ', text).strip() # Birden fazla boşluğu tek boşluğa indirir ve baştaki/sondaki boşlukları kaldırır.\n",
    "        return text\n",
    "\n",
    "    # 'news' sütunundaki her bir metne clean_text fonksiyonunu uygula ve yeni 'temiz_metin' sütunu oluştur.\n",
    "    df['temiz_metin'] = df[text_column].apply(clean_text)\n",
    "\n",
    "    # 3. Adım: Etiketleri sayısal hale dönüştür\n",
    "    # 'real' ve 'fake' string etiketlerini modelin anlayabileceği sayısal değerlere dönüştürmek için bir sözlük tanımlanır.\n",
    "    # 'real' (doğru) için 0, 'fake' (yanlış/sahte) için 1 atanır.\n",
    "    label_mapping = {'real': 0, 'fake': 1}\n",
    "\n",
    "    # Sadece 'real' veya 'fake' olan satırları filtrele\n",
    "    # Bu, 'type' sütununda başlık satırı gibi istenmeyen değerler varsa onları temizler.\n",
    "    df = df[df[label_column].isin(['real', 'fake'])].copy()\n",
    "\n",
    "    # 'type' sütunundaki etiketleri 'label_mapping'e göre sayısal 'etiket_id' sütununa dönüştür.\n",
    "    df['etiket_id'] = df[label_column].map(label_mapping)\n",
    "\n",
    "    # NaN (Not a Number) etiketleri varsa kaldır (veri setinde eksik veya eşleşmeyen etiketler olabilir).\n",
    "    # Bu adım, temizlenmiş etiket sütununda eksik değer bulunmamasını sağlar.\n",
    "    df.dropna(subset=['etiket_id'], inplace=True)\n",
    "    df['etiket_id'] = df['etiket_id'].astype(int) # Etiket ID'lerini tam sayı (integer) tipine çevir.\n",
    "\n",
    "    # Sadece modelin kullanacağı 'temiz_metin' ve 'etiket_id' sütunlarını seç.\n",
    "    # Hugging Face Transformer modelinin beklentisine uygun olarak sütun adlarını 'text' ve 'labels' olarak yeniden adlandır.\n",
    "    df = df[['temiz_metin', 'etiket_id']].rename(columns={'temiz_metin': 'text', 'etiket_id': 'labels'})\n",
    "\n",
    "    # DİAGNOSTİK KONTROL: train_test_split öncesi DataFrame'in boş olup olmadığını kontrol et\n",
    "    if len(df) == 0:\n",
    "        raise ValueError(\n",
    "            \"UYARI: Veri temizleme ve etiketleme adımlarından sonra DataFrame boş kaldı. \"\n",
    "            \"Bu, dosyanızdaki etiketlerin 'real' veya 'fake' dışında değerler içerdiği veya \"\n",
    "            \"metin/etiket sütunlarının boş olduğu anlamına gelebilir.\"\n",
    "            \"\\nŞu anki DataFrame boyutu: 0\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"\\nVeri temizleme sonrası DataFrame boyutu: {len(df)} satır.\")\n",
    "        print(\"DataFrame'in ilk 3 satırı (temizlenmiş):\\n\", df.head(3))\n",
    "        print(\"Etiket dağılımı (temizlenmiş):\\n\", df['labels'].value_counts())\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "\n",
    "    # 4. Adım: Veri Setini Eğitim, Doğrulama (Validation) ve Test Setlerine Ayır\n",
    "    # Veri setini %80 eğitim, %10 doğrulama ve %10 test oranlarında ayır.\n",
    "    # `random_state=42` sonuçların tekrarlanabilir olmasını sağlar.\n",
    "    # `stratify=df['labels']` etiket dağılımının her alt kümede aynı kalmasını sağlar, bu özellikle dengesiz veri setlerinde önemlidir.\n",
    "    train_val_df, test_df = train_test_split(df, test_size=0.10, random_state=42, stratify=df['labels'])\n",
    "    # Eğitim ve doğrulama setini kendi içinde tekrar bölerek %10'luk doğrulama setini oluştururuz.\n",
    "    train_df, val_df = train_test_split(train_val_df, test_size=0.10, random_state=42, stratify=train_val_df['labels'])\n",
    "\n",
    "    # Ayrılan setlerin boyutlarını (satır sayılarını) kontrol et\n",
    "    print(\"Eğitim seti boyutu:\", len(train_df))\n",
    "    print(\"Doğrulama (Validation) seti boyutu:\", len(val_df))\n",
    "    print(\"Test seti boyutu:\", len(test_df))\n",
    "\n",
    "    # Ayrılan setlerdeki etiket dağılımını kontrol et (stratify'ın işe yaradığını doğrulamak için)\n",
    "    print(\"\\nEğitim seti etiket dağılımı:\\n\", train_df['labels'].value_counts())\n",
    "    print(\"Doğrulama seti etiket dağılımı:\\n\", val_df['labels'].value_counts())\n",
    "    print(\"Test seti etiket dağılımı:\\n\", test_df['labels'].value_counts())\n",
    "\n",
    "    # Pandas DataFrame'lerini Hugging Face Dataset objelerine dönüştür.\n",
    "    train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "    val_dataset = Dataset.from_pandas(val_df, preserve_index=False)\n",
    "    test_dataset = Dataset.from_pandas(test_df, preserve_index=False)\n",
    "\n",
    "    # Dönüştürülen Dataset objelerinin ilk bilgilerini göster\n",
    "    print(\"\\nHugging Face Dataset formatında eğitim seti:\\n\", train_dataset)\n",
    "    print(\"Hugging Face Dataset formatında doğrulama seti:\\n\", val_dataset)\n",
    "    print(\"Hugging Face Dataset formatında test seti:\\n\", test_dataset)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Hata: '{file_path}' dosyası bulunamadı. Lütfen dosya adını ve yolunu kontrol edin.\")\n",
    "    print(\"Dosya adının 'fake_news.csv' olduğundan ve aynı dizinde bulunduğundan emin olun.\")\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(f\"Hata: '{file_path}' dosyası boş.\")\n",
    "except ValueError as ve:\n",
    "    print(f\"Veri setinin yapısında hata: {ve}\")\n",
    "except Exception as e:\n",
    "    print(f\"Beklenmeyen bir hata oluştu: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c443077b-b115-485c-8f33-8e6fe43e840a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veri setleri token'lara ayrılıyor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████████| 560/560 [00:00<00:00, 2694.23 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████| 63/63 [00:00<00:00, 2367.18 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████| 70/70 [00:00<00:00, 2467.07 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizasyon tamamlandı.\n",
      "Gereksiz sütunlar kaldırılıyor...\n",
      "Sütunlar kaldırıldı.\n",
      "\n",
      "Tokenize edilmiş eğitim seti:\n",
      " Dataset({\n",
      "    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 560\n",
      "})\n",
      "Tokenize edilmiş doğrulama seti:\n",
      " Dataset({\n",
      "    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 63\n",
      "})\n",
      "Tokenize edilmiş test seti:\n",
      " Dataset({\n",
      "    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 70\n",
      "})\n",
      "\n",
      "İlk token'lanmış eğitim örneği:\n",
      "{'labels': 0, 'input_ids': [2, 2048, 17153, 13349, 1992, 7539, 19709, 1990, 2012, 2605, 4857, 1973, 1996, 2339, 2045, 1023, 3288, 2839, 1024, 3956, 2330, 2570, 11866, 6440, 2975, 5512, 6301, 1973, 6922, 2165, 3765, 1990, 23963, 10910, 3176, 6101, 11920, 3926, 23721, 1025, 17153, 1048, 3445, 70, 1027, 6717, 2605, 4857, 2025, 3819, 28196, 6453, 6126, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer # Sadece AutoTokenizer sınıfını içe aktarıyoruz (diğerleri zaten yukarıda içe aktarılmış olmalı)\n",
    "\n",
    "# Tokenizer objesi, daha önceki hücrelerde 'model_name' değişkeni kullanılarak yüklendi.\n",
    "# Metinleri token'lara dönüştürmek için bir fonksiyon tanımlarız.\n",
    "# Bu fonksiyon, Hugging Face Dataset'in 'map' metodu ile kullanılacaktır.\n",
    "def tokenize_function(examples):\n",
    "    # 'examples[\"text\"]' içinde bulunan metinleri tokenizer ile işler.\n",
    "    # truncation=True: Metin, modelin maksimum giriş uzunluğunu (örneğin BERT için 512) aşarsa kesilir.\n",
    "    # padding=\"max_length\": Tüm metinleri aynı uzunluğa (max_length) kadar doldurur.\n",
    "    # max_length=256: Tüm metinlerin maksimum uzunluğunu 256 token olarak ayarlar.\n",
    "    # Bu değer, modelin kaldırabileceği ve projenin ihtiyaçlarına uygun bir dengedir.\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "\n",
    "# Metinleri Token'lara Ayırma \n",
    "print(\"Veri setleri token'lara ayrılıyor...\")\n",
    "# 'map' metodu, veri setindeki her örneğe tokenize_function'ı uygular.\n",
    "# 'batched=True' parametresi, fonksiyonun aynı anda birden fazla örnek üzerinde çalışmasını sağlayarak hızı artırır.\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True) # Doğrulama seti de token'lara ayrılır\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True) # Test seti de token'lara ayrılır\n",
    "print(\"Tokenizasyon tamamlandı.\")\n",
    "\n",
    "# Model girdisi olarak sadece 'input_ids', 'attention_mask' ve 'token_type_ids' (BERT için) gerekir.\n",
    "# Orijinal 'text' sütunu artık gereksizdir ve bellekten tasarruf etmek için kaldırılır.\n",
    "print(\"Gereksiz sütunlar kaldırılıyor...\")\n",
    "tokenized_train_dataset = tokenized_train_dataset.remove_columns([\"text\"])\n",
    "tokenized_val_dataset = tokenized_val_dataset.remove_columns([\"text\"]) # Doğrulama setinden de kaldırılır\n",
    "tokenized_test_dataset = tokenized_test_dataset.remove_columns([\"text\"]) # Test setinden de kaldırılır\n",
    "print(\"Sütunlar kaldırıldı.\")\n",
    "\n",
    "\n",
    "print(\"\\nTokenize edilmiş eğitim seti:\\n\", tokenized_train_dataset)\n",
    "print(\"Tokenize edilmiş doğrulama seti:\\n\", tokenized_val_dataset) # Doğrulama setinin bilgisini de göster\n",
    "print(\"Tokenize edilmiş test seti:\\n\", tokenized_test_dataset)\n",
    "\n",
    "# İlk token'lanmış eğitim örneğini incele (isteğe bağlı)\n",
    "# Bu, tokenizasyonun düzgün çalıştığını ve modelin beklediği formatta (input_ids, attention_mask, labels) olduğunu doğrulamak için faydalıdır.\n",
    "print(\"\\nİlk token'lanmış eğitim örneği:\")\n",
    "print(tokenized_train_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d5e4451-a67c-4c58-b961-98dc4a88a4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model eğitimi başlatılıyor...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 01:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.313441</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.277300</td>\n",
       "      <td>0.301536</td>\n",
       "      <td>0.920635</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.918033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.228700</td>\n",
       "      <td>0.337417</td>\n",
       "      <td>0.920635</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.915254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model eğitimi tamamlandı.\n",
      "\n",
      "Test seti üzerinde model değerlendiriliyor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Değerlendirme Sonuçları: {'eval_loss': 0.44081926345825195, 'eval_accuracy': 0.8714285714285714, 'eval_precision': 0.8378378378378378, 'eval_recall': 0.9117647058823529, 'eval_f1': 0.8732394366197183, 'eval_runtime': 1.4274, 'eval_samples_per_second': 49.04, 'eval_steps_per_second': 6.305, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import numpy as np # Sayısal işlemler için NumPy kütüphanesi\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score # Model performans metriklerini hesaplamak için\n",
    "\n",
    "# 1. Adım: Performans Metriklerini Tanımla (compute_metrics fonksiyonu)\n",
    "# Trainer tarafından her değerlendirme adımında çağrılacak olan metrik hesaplama fonksiyonu.\n",
    "# 'p' parametresi, modelin tahminlerini (predictions) ve gerçek etiketleri (label_ids) içerir.\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p.predictions, p.label_ids # Tahminleri ve gerçek etiketleri al\n",
    "    preds = np.argmax(predictions, axis=1) # En yüksek olasılığa sahip sınıfı tahmin olarak seç (logitlerden sınıf indeksine dönüştür)\n",
    "\n",
    "    # İkili sınıflandırma için (0: doğru, 1: yanlış) kesinlik, geri çağırma ve F1 skorunu hesapla.\n",
    "    # 'average='binary'' ikili sınıflandırma için metriklerin hesaplandığını belirtir.\n",
    "    # 'pos_label=1' pozitif sınıfın (yani 'yanlış' haberin) 1 olduğunu belirtir.\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary', pos_label=1)\n",
    "    acc = accuracy_score(labels, preds) # Doğruluk (Accuracy) oranını hesapla\n",
    "    \n",
    "    # Hesaplanan metrikleri bir sözlük olarak döndür. Trainer bu formatı bekler.\n",
    "    return {'accuracy': acc, 'precision': precision, 'recall': recall, 'f1': f1}\n",
    "\n",
    "# 2. Adım: Eğitim Argümanlarını Belirle\n",
    "# TrainingArguments sınıfı, eğitim sürecinin nasıl yapılandırılacağını belirler.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\", # Model çıktıları ve checkpoint'lerin kaydedileceği dizin\n",
    "    eval_strategy=\"epoch\", # Değerlendirmeyi her epoch sonunda yap\n",
    "    save_strategy=\"epoch\", # Her epoch sonunda model checkpoint'ini kaydet\n",
    "    learning_rate=1e-5, # Modelin öğrenme oranı (BERT ince ayarı için genellikle küçük bir değer kullanılır)\n",
    "    per_device_train_batch_size=8, # Her GPU/CPU cihazı için eğitim batch boyutu\n",
    "    per_device_eval_batch_size=8, # Her GPU/CPU cihazı için değerlendirme batch boyutu\n",
    "    num_train_epochs=3, # Toplam eğitim epoch sayısı\n",
    "    weight_decay=0.01, # Ağırlık bozunumu (L2 regülarizasyonu), aşırı öğrenmeyi engellemeye yardımcı olur\n",
    "    logging_dir='./logs', # Logların kaydedileceği dizin\n",
    "    logging_steps=100, # Loglama bilgilerini (kayıp vb.) kaç adımda bir yazdırılacağını belirler\n",
    "    # save_total_limit=2, # Yorum satırı: Kaydedilecek en fazla checkpoint sayısını belirler (örn: en iyi 2 model)\n",
    "    load_best_model_at_end=True, # Eğitimin sonunda doğrulama setinde en iyi performansı gösteren modeli yükle\n",
    "    metric_for_best_model=\"f1\", # En iyi modeli belirlemek için kullanılacak metrik\n",
    "    greater_is_better=True, # Seçilen metrik (f1) için daha yüksek değerin daha iyi olduğunu belirtir\n",
    ")\n",
    "\n",
    "# 3. Adım: Trainer Objesini Tanımla\n",
    "# Trainer sınıfı, PyTorch modellerini eğitmek ve değerlendirmek için yüksek seviyeli bir API sağlar.\n",
    "trainer = Trainer(\n",
    "    model=model, # Eğitilecek model\n",
    "    args=training_args, # Eğitim argümanları\n",
    "    train_dataset=tokenized_train_dataset, # Eğitim veri seti (daha önce token'lara ayrılmış)\n",
    "    eval_dataset=tokenized_val_dataset, # Doğrulama veri seti (eğitim sırasında performansı izlemek için)\n",
    "    compute_metrics=compute_metrics, # Metrik hesaplama fonksiyonu\n",
    ")\n",
    "\n",
    "# 4. Adım: Modeli Eğit!\n",
    "print(\"\\nModel eğitimi başlatılıyor...\")\n",
    "trainer.train() # Eğitim sürecini başlatır\n",
    "print(\"Model eğitimi tamamlandı.\")\n",
    "\n",
    "# 5. Adım: Modelin Son Değerlendirmesini Yap\n",
    "print(\"\\nTest seti üzerinde model değerlendiriliyor...\")\n",
    "# Eğitilmiş modeli daha önce ayrılmış test seti üzerinde değerlendirir.\n",
    "# Bu, modelin yeni, daha önce görmediği veriler üzerindeki genelleme yeteneğini ölçer.\n",
    "evaluation_results = trainer.evaluate(eval_dataset=tokenized_test_dataset)\n",
    "print(\"Değerlendirme Sonuçları:\", evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "013d5ac2-1f97-4413-8b11-841056c36344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Örnek Metinler Üzerinde Tahminler ---\n",
      "\n",
      "Metin: 'Bilim insanları Mars'ta devasa bir uzaylı üssü keşfetti.'\n",
      "Yanlış Olma Olasılığı (Risk Puanı): 0.9048\n",
      "Tahmini Etiket: Yanlış\n",
      ">> Bu haberin yanlış olma olasılığı YÜKSEK. Lütfen güvenilir kaynaklardan teyit edin (örn. teyit.org).\n",
      "\n",
      "Metin: '2024 olimpiyatlarına Paris ev sahipliği yapacak.'\n",
      "Yanlış Olma Olasılığı (Risk Puanı): 0.2556\n",
      "Tahmini Etiket: Doğru\n",
      ">> Bu haberin doğru olma olasılığı YÜKSEK görünüyor. Yine de her zaman kritik düşünün.\n",
      "\n",
      "Metin: 'Dünya'nın çekirdeğindeki manyetik alan, bu hafta itibarıyla yön değiştirerek tüm pusulaları bozdu.'\n",
      "Yanlış Olma Olasılığı (Risk Puanı): 0.8493\n",
      "Tahmini Etiket: Yanlış\n",
      ">> Bu haberin yanlış olma olasılığı YÜKSEK. Lütfen güvenilir kaynaklardan teyit edin (örn. teyit.org).\n",
      "\n",
      "Metin: 'Birleşmiş Milletler Güvenlik Konseyi, Suriye'ye yönelik insani yardımın süresini altı ay daha uzatma kararı aldı.'\n",
      "Yanlış Olma Olasılığı (Risk Puanı): 0.0142\n",
      "Tahmini Etiket: Doğru\n",
      ">> Bu haberin doğru olma olasılığı YÜKSEK görünüyor. Yine de her zaman kritik düşünün.\n",
      "\n",
      "Metin: 'Güneş Sistemi'nde yeni bir gezegen bulundu ve adı 'Patoz' konuldu.'\n",
      "Yanlış Olma Olasılığı (Risk Puanı): 0.8092\n",
      "Tahmini Etiket: Yanlış\n",
      ">> Bu haberin yanlış olma olasılığı YÜKSEK. Lütfen güvenilir kaynaklardan teyit edin (örn. teyit.org).\n",
      "\n",
      "Metin: 'Avrupa Merkez Bankası, artan enflasyonla mücadele kapsamında faiz oranlarında 25 baz puanlık artışa gitti.'\n",
      "Yanlış Olma Olasılığı (Risk Puanı): 0.7374\n",
      "Tahmini Etiket: Yanlış\n",
      ">> Bu haberin yanlış olma olasılığı YÜKSEK. Lütfen güvenilir kaynaklardan teyit edin (örn. teyit.org).\n",
      "\n",
      "Metin: 'Yeni yapılan bir araştırmaya göre, gece 4 saat uyuyanlar, 8 saat uyuyanlara göre daha uzun yaşıyor.'\n",
      "Yanlış Olma Olasılığı (Risk Puanı): 0.9850\n",
      "Tahmini Etiket: Yanlış\n",
      ">> Bu haberin yanlış olma olasılığı YÜKSEK. Lütfen güvenilir kaynaklardan teyit edin (örn. teyit.org).\n",
      "\n",
      "Metin: 'Diplomatlar, Afrika'daki kuraklık kriziyle mücadele için uluslararası bir fon oluşturulması konusunda anlaşmaya vardı.'\n",
      "Yanlış Olma Olasılığı (Risk Puanı): 0.0478\n",
      "Tahmini Etiket: Doğru\n",
      ">> Bu haberin doğru olma olasılığı YÜKSEK görünüyor. Yine de her zaman kritik düşünün.\n",
      "\n",
      "Metin: 'Hükümet, 2025'ten itibaren tüm araçlarda suyla çalışan motorların kullanılmasını zorunlu hale getirecek.'\n",
      "Yanlış Olma Olasılığı (Risk Puanı): 0.8999\n",
      "Tahmini Etiket: Yanlış\n",
      ">> Bu haberin yanlış olma olasılığı YÜKSEK. Lütfen güvenilir kaynaklardan teyit edin (örn. teyit.org).\n",
      "\n",
      "Metin: 'Uzay araştırmacıları, Ay'ın üzerinde 1500 yıl önce inşa edilmiş piramit benzeri yapılar keşfetti.'\n",
      "Yanlış Olma Olasılığı (Risk Puanı): 0.7706\n",
      "Tahmini Etiket: Yanlış\n",
      ">> Bu haberin yanlış olma olasılığı YÜKSEK. Lütfen güvenilir kaynaklardan teyit edin (örn. teyit.org).\n",
      "\n",
      "--- Siber Okuryazarlık İpuçları ---\n",
      "1. Haber başlıkları çok iddialı veya duygusal ise şüphelenin.\n",
      "2. Haberin kaynağını (yazar, yayıncı site) kontrol edin. Güvenilir mi?\n",
      "3. Haberin tarihi önemli mi? Eski bir haber güncel gibi mi sunuluyor?\n",
      "4. Farklı, bilinen haber kaynaklarından aynı bilgiyi teyit etmeye çalışın.\n",
      "5. Yazım ve dilbilgisi hatalarına dikkat edin. Profesyonel yayınlarda bu tür hatalar nadirdir.\n",
      "6. Görselleri tersine görsel arama ile kontrol edin. Farklı bir bağlamda kullanılmış olabilirler.\n"
     ]
    }
   ],
   "source": [
    "import torch # Derin öğrenme operasyonları için PyTorch kütüphanesi\n",
    "\n",
    "# 1. Tahmin Fonksiyonu Yaz\n",
    "# Verilen bir metnin yanlış olma olasılığını tahmin eden fonksiyon.\n",
    "# 'clean_text' fonksiyonu ve 'tokenizer' ile 'model' objeleri daha önceki hücrelerde tanımlandı.\n",
    "def predict_fake_news(text, model, tokenizer, max_length=256):\n",
    "    # Tahmin yapılacak metni, eğitim verisiyle aynı şekilde temizle.\n",
    "    # 'clean_text' fonksiyonunun bu hücreye erişilebilir olması gerekmektedir.\n",
    "    cleaned_text = clean_text(text) \n",
    "\n",
    "    # Temizlenmiş metni tokenize et.\n",
    "    # `return_tensors=\"pt\"`: PyTorch tensorları olarak çıktı döndür.\n",
    "    # `truncation=True`: Metin max_length'ten uzunsa kes.\n",
    "    # `padding=\"max_length\"`: Metin max_length'ten kısaysa doldur.\n",
    "    # `max_length=max_length`: Tokenize edilmiş metnin maksimum uzunluğu.\n",
    "    inputs = tokenizer(cleaned_text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "\n",
    "    # DİKKAT: Buradaki girinti hatası düzeltildi!\n",
    "    if torch.cuda.is_available():\n",
    "        # Girdileri (input_ids, attention_mask) GPU'ya taşı\n",
    "        inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "        # Modeli GPU'ya taşı (eğer henüz taşınmadıysa veya CPU'ya geri döndüyse)\n",
    "        # Trainer genellikle modeli GPU'da bırakır, ancak bağımsız bir tahmin fonksiyonunda bu kontrolü yapmak iyidir.\n",
    "        if model.device.type != 'cuda': \n",
    "            model.to('cuda')\n",
    "            \n",
    "    # Model ile tahmin yap.\n",
    "    # `torch.no_grad()`: Tahmin yaparken gradyan hesaplamalarını devre dışı bırakır,\n",
    "    # bu da bellek kullanımını azaltır ve çıkarım hızını artırır.\n",
    "    with torch.no_grad(): \n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Modelin ham çıktıları (logitler) alınır ve olasılıklara dönüştürülür.\n",
    "    # `torch.softmax(logits, dim=1)`: Logitleri olasılık dağılımına çevirir.\n",
    "    # `dim=1`: Sınıf boyutunda softmax uygula (yani her örnek için sınıf olasılıkları).\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.softmax(logits, dim=1)\n",
    "\n",
    "    # Yanlış olma olasılığı, 'fake' etiketi için atadığımız indeks olan 1'e karşılık gelir.\n",
    "    # `[:, 1]` ikinci sınıfın (yani 'yanlış' sınıfının) olasılığını seçer.\n",
    "    # `.item()`: Tensor içindeki tek bir değeri Python sayısına dönüştürür.\n",
    "    fake_prob = probabilities[:, 1].item()\n",
    "    \n",
    "    return fake_prob\n",
    "\n",
    "# 2. Örnek Metinler Üzerinde Tahminleri Test Et\n",
    "print(\"\\n--- Örnek Metinler Üzerinde Tahminler ---\")\n",
    "\n",
    "# Modelin performansını göstermek için kullanılacak örnek metinler listesi.\n",
    "test_texts = [\n",
    "    \"Bilim insanları Mars'ta devasa bir uzaylı üssü keşfetti.\", # Yanlış olması beklenen (kurgu)\n",
    "    \"2024 olimpiyatlarına Paris ev sahipliği yapacak.\", # Doğru olması beklenen (gerçek bilgi)\n",
    "    \"Dünya'nın çekirdeğindeki manyetik alan, bu hafta itibarıyla yön değiştirerek tüm pusulaları bozdu.\", #Yanlış\n",
    "    \"Birleşmiş Milletler Güvenlik Konseyi, Suriye'ye yönelik insani yardımın süresini altı ay daha uzatma kararı aldı.\", #Doğru\n",
    "    \"Güneş Sistemi'nde yeni bir gezegen bulundu ve adı 'Patoz' konuldu.\", # Yanlış olması beklenen (kurgu)\n",
    "    \"Avrupa Merkez Bankası, artan enflasyonla mücadele kapsamında faiz oranlarında 25 baz puanlık artışa gitti.\", #Doğru\n",
    "    \"Yeni yapılan bir araştırmaya göre, gece 4 saat uyuyanlar, 8 saat uyuyanlara göre daha uzun yaşıyor.\",#Yanlış\n",
    "    \"Diplomatlar, Afrika'daki kuraklık kriziyle mücadele için uluslararası bir fon oluşturulması konusunda anlaşmaya vardı.\" ,#Doğru\n",
    "    \"Hükümet, 2025'ten itibaren tüm araçlarda suyla çalışan motorların kullanılmasını zorunlu hale getirecek.\" ,#Yanlış\n",
    "    \"Uzay araştırmacıları, Ay'ın üzerinde 1500 yıl önce inşa edilmiş piramit benzeri yapılar keşfetti.\"#Yanlış\n",
    "]\n",
    "\n",
    "# Sayısal etiket ID'lerini okunabilir metin etiketlerine dönüştürmek için sözlük.\n",
    "id_to_label = {0: 'Doğru', 1: 'Yanlış'}\n",
    "\n",
    "# Her bir örnek metin üzerinde tahmin yap ve sonuçları göster.\n",
    "for text in test_texts:\n",
    "    # predict_fake_news fonksiyonunu çağırarak yanlış olma olasılığını al.\n",
    "    risk_score = predict_fake_news(text, model, tokenizer)\n",
    "    # Risk puanı 0.5'ten büyükse 'Yanlış' (1), değilse 'Doğru' (0) olarak sınıflandır.\n",
    "    predicted_label_id = 1 if risk_score > 0.5 else 0 \n",
    "    predicted_label = id_to_label[predicted_label_id] # Sayısal etiketi metin etikete dönüştür.\n",
    "\n",
    "    print(f\"\\nMetin: '{text}'\")\n",
    "    print(f\"Yanlış Olma Olasılığı (Risk Puanı): {risk_score:.4f}\") # Olasılığı 4 ondalık basamakla yazdır.\n",
    "    print(f\"Tahmini Etiket: {predicted_label}\")\n",
    "\n",
    "    # Öneri Mekanizması: Risk puanına göre kullanıcıya farklı uyarılarda bulun.\n",
    "    if risk_score > 0.7: # Yüksek riskli (örn. %70 üzeri)\n",
    "        print(\">> Bu haberin yanlış olma olasılığı YÜKSEK. Lütfen güvenilir kaynaklardan teyit edin (örn. teyit.org).\")\n",
    "    elif risk_score > 0.4: # Orta riskli (örn. %40-70 arası)\n",
    "        print(\">> Bu haberin doğruluğunu teyit etmeniz ÖNERİLİR. Kaynağını kontrol edin.\")\n",
    "    else: # Düşük riskli (örn. %40 altı)\n",
    "        print(\">> Bu haberin doğru olma olasılığı YÜKSEK görünüyor. Yine de her zaman kritik düşünün.\")\n",
    "\n",
    "# Siber Okuryazarlık İpuçları (Metinsel)\n",
    "# Kullanıcıların dezenformasyonu tanıma ve eleştirel düşünme becerilerini geliştirmelerine yardımcı olacak pratik ipuçları.\n",
    "print(\"\\n--- Siber Okuryazarlık İpuçları ---\")\n",
    "print(\"1. Haber başlıkları çok iddialı veya duygusal ise şüphelenin.\")\n",
    "print(\"2. Haberin kaynağını (yazar, yayıncı site) kontrol edin. Güvenilir mi?\")\n",
    "print(\"3. Haberin tarihi önemli mi? Eski bir haber güncel gibi mi sunuluyor?\")\n",
    "print(\"4. Farklı, bilinen haber kaynaklarından aynı bilgiyi teyit etmeye çalışın.\")\n",
    "print(\"5. Yazım ve dilbilgisi hatalarına dikkat edin. Profesyonel yayınlarda bu tür hatalar nadirdir.\")\n",
    "print(\"6. Görselleri tersine görsel arama ile kontrol edin. Farklı bir bağlamda kullanılmış olabilirler.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3bb56d-4362-4947-b991-1c59b944f800",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
